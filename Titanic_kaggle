{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "235a3e6c",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-08-16T04:12:20.732729Z",
     "iopub.status.busy": "2025-08-16T04:12:20.732407Z",
     "iopub.status.idle": "2025-08-16T04:12:22.570199Z",
     "shell.execute_reply": "2025-08-16T04:12:22.569017Z"
    },
    "papermill": {
     "duration": 1.843853,
     "end_time": "2025-08-16T04:12:22.572052",
     "exception": false,
     "start_time": "2025-08-16T04:12:20.728199",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/titanic/train.csv\n",
      "/kaggle/input/titanic/test.csv\n",
      "/kaggle/input/titanic/gender_submission.csv\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a5b81c39",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-16T04:12:22.579650Z",
     "iopub.status.busy": "2025-08-16T04:12:22.579244Z",
     "iopub.status.idle": "2025-08-16T05:04:18.079443Z",
     "shell.execute_reply": "2025-08-16T05:04:18.078421Z"
    },
    "papermill": {
     "duration": 3115.50625,
     "end_time": "2025-08-16T05:04:18.081054",
     "exception": false,
     "start_time": "2025-08-16T04:12:22.574804",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ XGBoost available\n",
      "✅ LightGBM available\n",
      "✅ CatBoost available\n",
      "🚢 Titanic Enhanced Pipeline - Target: 0.80+ Accuracy\n",
      "Train shape: (891, 12) | Test shape: (418, 11)\n",
      "⚙️ Processing features...\n",
      "✅ Total features: 34\n",
      "🤖 Setting up models...\n",
      "✅ 13 models configured\n",
      "\n",
      "🔍 Cross-Validation Results:\n",
      "------------------------------------------------------------\n",
      "Testing LogReg... 0.81813 ± 0.02094\n",
      "Testing LogReg_Poly... "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/sklearn/feature_selection/_univariate_selection.py:112: UserWarning: Features [24 31 52 53 54] are constant.\n",
      "  warnings.warn(\"Features %s are constant.\" % constant_features_idx, UserWarning)\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: invalid value encountered in divide\n",
      "  f = msb / msw\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/feature_selection/_univariate_selection.py:112: UserWarning: Features [24 31 52 53 54] are constant.\n",
      "  warnings.warn(\"Features %s are constant.\" % constant_features_idx, UserWarning)\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: invalid value encountered in divide\n",
      "  f = msb / msw\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/feature_selection/_univariate_selection.py:112: UserWarning: Features [24 31 52 53 54] are constant.\n",
      "  warnings.warn(\"Features %s are constant.\" % constant_features_idx, UserWarning)\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: invalid value encountered in divide\n",
      "  f = msb / msw\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/feature_selection/_univariate_selection.py:112: UserWarning: Features [24 31 52 53 54] are constant.\n",
      "  warnings.warn(\"Features %s are constant.\" % constant_features_idx, UserWarning)\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: invalid value encountered in divide\n",
      "  f = msb / msw\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/feature_selection/_univariate_selection.py:112: UserWarning: Features [24 31 52 53 54] are constant.\n",
      "  warnings.warn(\"Features %s are constant.\" % constant_features_idx, UserWarning)\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: invalid value encountered in divide\n",
      "  f = msb / msw\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/feature_selection/_univariate_selection.py:112: UserWarning: Features [24 31 52 53 54] are constant.\n",
      "  warnings.warn(\"Features %s are constant.\" % constant_features_idx, UserWarning)\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: invalid value encountered in divide\n",
      "  f = msb / msw\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/feature_selection/_univariate_selection.py:112: UserWarning: Features [24 31 52 53 54] are constant.\n",
      "  warnings.warn(\"Features %s are constant.\" % constant_features_idx, UserWarning)\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: invalid value encountered in divide\n",
      "  f = msb / msw\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/feature_selection/_univariate_selection.py:112: UserWarning: Features [24 31 52 53 54] are constant.\n",
      "  warnings.warn(\"Features %s are constant.\" % constant_features_idx, UserWarning)\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: invalid value encountered in divide\n",
      "  f = msb / msw\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/feature_selection/_univariate_selection.py:112: UserWarning: Features [24 31 52 53 54] are constant.\n",
      "  warnings.warn(\"Features %s are constant.\" % constant_features_idx, UserWarning)\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: invalid value encountered in divide\n",
      "  f = msb / msw\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/feature_selection/_univariate_selection.py:112: UserWarning: Features [24 31 52 53 54] are constant.\n",
      "  warnings.warn(\"Features %s are constant.\" % constant_features_idx, UserWarning)\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: invalid value encountered in divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.82149 ± 0.02662\n",
      "Testing SVM_RBF... 0.81477 ± 0.03473\n",
      "Testing RandomForest... 0.83159 ± 0.03239\n",
      "Testing ExtraTrees... 0.81926 ± 0.02969\n",
      "Testing GradBoost... 0.83828 ± 0.04626\n",
      "Testing AdaBoost... 0.80918 ± 0.03772\n",
      "Testing KNN... 0.78672 ± 0.04191\n",
      "Testing NB... 0.45005 ± 0.02848\n",
      "Testing MLP... 0.82479 ± 0.05314\n",
      "Testing XGB... 0.82144 ± 0.04178\n",
      "Testing LGBM... 0.83272 ± 0.02353\n",
      "Testing CatBoost... 0.83944 ± 0.03039\n",
      "\n",
      "🏆 Best single model: CatBoost (0.83944)\n",
      "\n",
      "🔗 Building Advanced Ensembles:\n",
      "------------------------------------------------------------\n",
      "Top models for stacking:\n",
      "  CatBoost: 0.83944\n",
      "  GradBoost: 0.83828\n",
      "  LGBM: 0.83272\n",
      "  RandomForest: 0.83159\n",
      "  MLP: 0.82479\n",
      "  LogReg_Poly: 0.82149\n",
      "Testing Stack_LR... 0.83494 ± 0.02641\n",
      "Testing Stack_RF... "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/pandas/core/algorithms.py:1743: DeprecationWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  return lib.map_infer(values, mapper, convert=convert)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.83831 ± 0.03285\n",
      "Testing Voting... 0.83493 ± 0.03083\n",
      "\n",
      "🏆 BEST MODEL: CatBoost (0.83944)\n",
      "Using single model: CatBoost\n",
      "\n",
      "⚡ Training final model...\n",
      "✅ Training complete!\n",
      "\n",
      "🎯 Generating Predictions:\n",
      "------------------------------------------------------------\n",
      "✅ Main submission created: 37.32% survival rate\n",
      "✅ Conservative submission: 38.28% survival rate\n",
      "✅ Aggressive submission: 36.36% survival rate\n",
      "\n",
      "📊 FINAL PERFORMANCE ANALYSIS:\n",
      "======================================================================\n",
      "Expected Kaggle Score: 0.83944\n",
      "Target Achievement: ✅ ACHIEVED!\n",
      "Holdout Validation: 0.81564\n",
      "Training survival rate: 38.38%\n",
      "\n",
      "🎯 SUBMISSION FILES CREATED:\n",
      "  1. submission.csv (Primary recommendation)\n",
      "  2. submission_conservative.csv (Lower threshold)\n",
      "  3. submission_aggressive.csv (Higher threshold)\n",
      "\n",
      "🚀 Pipeline complete! Expected performance: 80%+ accuracy! 🎉\n",
      "\n",
      "📈 Top 10 Feature Importances:\n",
      "        feature  importance\n",
      "            Age    8.486875\n",
      "       Title_Mr    8.443509\n",
      "       Sex_male    7.259036\n",
      "     NameLength    7.065909\n",
      "       AgeClass    6.825613\n",
      "   TicketNumber    6.795972\n",
      "  FarePerPerson    5.988603\n",
      "   WomanOrChild    5.513183\n",
      "      ClassFare    4.123944\n",
      "TicketGroupSize    3.807130\n",
      "\n",
      "======================================================================\n",
      "🏁 PIPELINE EXECUTION COMPLETE!\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# Titanic Survival Prediction – Enhanced Pipeline + Advanced Stacking (Target: 0.80+)\n",
    "# Author: Enhanced Kaggle Solution\n",
    "# Competition: Titanic - Machine Learning from Disaster\n",
    "\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler, RobustScaler, PolynomialFeatures\n",
    "from sklearn.impute import SimpleImputer, KNNImputer\n",
    "from sklearn.model_selection import StratifiedKFold, cross_val_score, GridSearchCV, train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression, RidgeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import (RandomForestClassifier, GradientBoostingClassifier, \n",
    "                             StackingClassifier, VotingClassifier, ExtraTreesClassifier,\n",
    "                             AdaBoostClassifier)\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "# Optional: Advanced models\n",
    "try:\n",
    "    from xgboost import XGBClassifier\n",
    "    HAS_XGB = True\n",
    "    print(\"✅ XGBoost available\")\n",
    "except ImportError:\n",
    "    HAS_XGB = False\n",
    "    print(\"❌ XGBoost not available\")\n",
    "\n",
    "try:\n",
    "    from lightgbm import LGBMClassifier\n",
    "    HAS_LGBM = True\n",
    "    print(\"✅ LightGBM available\")\n",
    "except ImportError:\n",
    "    HAS_LGBM = False\n",
    "    print(\"❌ LightGBM not available\")\n",
    "\n",
    "try:\n",
    "    from catboost import CatBoostClassifier\n",
    "    HAS_CATBOOST = True\n",
    "    print(\"✅ CatBoost available\")\n",
    "except ImportError:\n",
    "    HAS_CATBOOST = False\n",
    "    print(\"❌ CatBoost not available\")\n",
    "\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "\n",
    "# =============================================================================\n",
    "# 1) LOAD DATA\n",
    "# =============================================================================\n",
    "print(\"🚢 Titanic Enhanced Pipeline - Target: 0.80+ Accuracy\")\n",
    "\n",
    "# Kaggle input paths\n",
    "train_df = pd.read_csv('/kaggle/input/titanic/train.csv')\n",
    "test_df = pd.read_csv('/kaggle/input/titanic/test.csv')\n",
    "\n",
    "print(f\"Train shape: {train_df.shape} | Test shape: {test_df.shape}\")\n",
    "\n",
    "# =============================================================================\n",
    "# 2) ADVANCED FEATURE ENGINEERING\n",
    "# =============================================================================\n",
    "\n",
    "def advanced_feature_engineering(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    g = df.copy()\n",
    "\n",
    "    # Title extraction with more sophisticated mapping\n",
    "    g['Title'] = g['Name'].str.extract(r' ([A-Za-z]+)\\.', expand=False)\n",
    "    \n",
    "    # Advanced title mapping based on survival patterns\n",
    "    title_map = {\n",
    "        'Mr': 'Mr',           # Low survival\n",
    "        'Mrs': 'Mrs',         # High survival\n",
    "        'Miss': 'Miss',       # High survival\n",
    "        'Master': 'Master',   # High survival (young boys)\n",
    "        'Dr': 'Officer',      # Mixed survival\n",
    "        'Rev': 'Officer',     # Low survival\n",
    "        'Col': 'Officer',     # Mixed survival\n",
    "        'Major': 'Officer',   # Mixed survival\n",
    "        'Capt': 'Officer',    # Low survival\n",
    "        'Don': 'Royalty',     # High survival\n",
    "        'Lady': 'Royalty',    # High survival\n",
    "        'Countess': 'Royalty',# High survival\n",
    "        'Sir': 'Royalty',     # High survival\n",
    "        'Jonkheer': 'Royalty',# Mixed survival\n",
    "        'Mme': 'Mrs',         # High survival\n",
    "        'Ms': 'Miss',         # High survival\n",
    "        'Mlle': 'Miss',       # High survival\n",
    "        'Dona': 'Royalty'     # High survival\n",
    "    }\n",
    "    g['Title'] = g['Title'].map(title_map).fillna('Rare')\n",
    "\n",
    "    # Name features\n",
    "    g['NameLength'] = g['Name'].str.len()\n",
    "    g['NameWordCount'] = g['Name'].str.split().str.len()\n",
    "    \n",
    "    # Family-based features\n",
    "    g['FamilySize'] = g['SibSp'] + g['Parch'] + 1\n",
    "    g['IsAlone'] = (g['FamilySize'] == 1).astype(int)\n",
    "    g['SmallFamily'] = ((g['FamilySize'] >= 2) & (g['FamilySize'] <= 4)).astype(int)\n",
    "    g['LargeFamily'] = (g['FamilySize'] > 4).astype(int)\n",
    "\n",
    "    # Age categories with domain knowledge\n",
    "    age_median = g['Age'].median()\n",
    "    g['IsChild'] = (g['Age'] <= 12).astype(float)\n",
    "    g['IsElderly'] = (g['Age'] >= 60).astype(float)\n",
    "    g['IsYoungAdult'] = ((g['Age'] > 12) & (g['Age'] <= 35)).astype(float)\n",
    "    \n",
    "    # Fill NaN values for age categories\n",
    "    g['IsChild'] = g['IsChild'].fillna(0)\n",
    "    g['IsElderly'] = g['IsElderly'].fillna(0)\n",
    "    g['IsYoungAdult'] = g['IsYoungAdult'].fillna(0)\n",
    "\n",
    "    # Deck from Cabin\n",
    "    g['Deck'] = g['Cabin'].astype(str).str[0]\n",
    "    g['Deck'] = g['Deck'].replace({'n': 'U'})  # NaN handling\n",
    "    g['HasCabin'] = (~g['Cabin'].isna()).astype(int)\n",
    "    g['CabinNumber'] = g['Cabin'].astype(str).str.extract('(\\d+)', expand=False)\n",
    "    g['CabinNumber'] = pd.to_numeric(g['CabinNumber'], errors='coerce').fillna(0)\n",
    "    \n",
    "    # Multiple cabins (wealth indicator)\n",
    "    g['MultipleCabins'] = g['Cabin'].fillna('').apply(lambda x: len(str(x).split()) if pd.notna(x) and x != '' else 0).astype(int)\n",
    "\n",
    "    # Ticket features\n",
    "    g['TicketPrefix'] = g['Ticket'].str.extract('([A-Za-z/\\.]+)', expand=False).fillna('None')\n",
    "    g['TicketNumber'] = g['Ticket'].str.extract('(\\d+)', expand=False)\n",
    "    g['TicketNumber'] = pd.to_numeric(g['TicketNumber'], errors='coerce').fillna(0)\n",
    "    \n",
    "    # Ticket group size - fixed implementation\n",
    "    ticket_counts = g['Ticket'].value_counts().to_dict()\n",
    "    g['TicketGroupSize'] = g['Ticket'].map(ticket_counts)\n",
    "    g['SharedTicket'] = (g['TicketGroupSize'] > 1).astype(int)\n",
    "\n",
    "    # Fare features\n",
    "    fare_median = g['Fare'].median()\n",
    "    g['Fare'] = g['Fare'].fillna(fare_median)\n",
    "    g['FarePerPerson'] = g['Fare'] / g['FamilySize'].replace(0, 1)\n",
    "    \n",
    "    # Safe fare binning\n",
    "    try:\n",
    "        g['FareBin'] = pd.qcut(g['Fare'], q=5, labels=False, duplicates='drop')\n",
    "    except:\n",
    "        g['FareBin'] = pd.cut(g['Fare'], bins=5, labels=False)\n",
    "    \n",
    "    g['HighFare'] = (g['Fare'] > g['Fare'].quantile(0.8)).astype(float)\n",
    "\n",
    "    # Interaction features\n",
    "    g['WomanOrChild'] = ((g['Sex'] == 'female') | (g['Age'] <= 16)).astype(float)\n",
    "    g['WomanOrChild'] = g['WomanOrChild'].fillna(0)  # Handle NaN ages\n",
    "    \n",
    "    g['MaleThirdClass'] = ((g['Sex'] == 'male') & (g['Pclass'] == 3)).astype(int)\n",
    "    g['FemaleFirstClass'] = ((g['Sex'] == 'female') & (g['Pclass'] == 1)).astype(int)\n",
    "    g['ChildFirstClass'] = ((g['Age'] <= 16) & (g['Pclass'] == 1)).astype(float)\n",
    "    g['ChildFirstClass'] = g['ChildFirstClass'].fillna(0)\n",
    "    \n",
    "    # Class-Fare interaction\n",
    "    g['ClassFare'] = g['Pclass'] * g['Fare']\n",
    "    \n",
    "    # Age-Class interaction\n",
    "    g['AgeClass'] = g['Age'].fillna(age_median) * g['Pclass']\n",
    "\n",
    "    return g\n",
    "\n",
    "print(\"⚙️ Processing features...\")\n",
    "train_eng = advanced_feature_engineering(train_df)\n",
    "test_eng = advanced_feature_engineering(test_df)\n",
    "\n",
    "TARGET = 'Survived'\n",
    "FEATURES = [\n",
    "    # Original features\n",
    "    'Pclass', 'Sex', 'Age', 'SibSp', 'Parch', 'Fare', 'Embarked',\n",
    "    # Engineered features\n",
    "    'Title', 'NameLength', 'NameWordCount',\n",
    "    'FamilySize', 'IsAlone', 'SmallFamily', 'LargeFamily',\n",
    "    'IsChild', 'IsElderly', 'IsYoungAdult',\n",
    "    'Deck', 'HasCabin', 'CabinNumber', 'MultipleCabins',\n",
    "    'TicketPrefix', 'TicketNumber', 'TicketGroupSize', 'SharedTicket',\n",
    "    'FarePerPerson', 'FareBin', 'HighFare',\n",
    "    'WomanOrChild', 'MaleThirdClass', 'FemaleFirstClass', 'ChildFirstClass',\n",
    "    'ClassFare', 'AgeClass'\n",
    "]\n",
    "\n",
    "X = train_eng[FEATURES]\n",
    "y = train_eng[TARGET]\n",
    "X_test = test_eng[FEATURES]\n",
    "\n",
    "print(f\"✅ Total features: {len(FEATURES)}\")\n",
    "\n",
    "# =============================================================================\n",
    "# 3) ADVANCED PREPROCESSORS\n",
    "# =============================================================================\n",
    "\n",
    "# Separate numeric and categorical features\n",
    "num_feats = [\n",
    "    'Age', 'SibSp', 'Parch', 'Fare', 'NameLength', 'NameWordCount',\n",
    "    'FamilySize', 'IsAlone', 'SmallFamily', 'LargeFamily',\n",
    "    'IsChild', 'IsElderly', 'IsYoungAdult', 'HasCabin', 'CabinNumber',\n",
    "    'MultipleCabins', 'TicketNumber', 'TicketGroupSize', 'SharedTicket',\n",
    "    'FarePerPerson', 'FareBin', 'HighFare', 'WomanOrChild',\n",
    "    'MaleThirdClass', 'FemaleFirstClass', 'ChildFirstClass',\n",
    "    'ClassFare', 'AgeClass'\n",
    "]\n",
    "\n",
    "cat_feats = ['Pclass', 'Sex', 'Embarked', 'Title', 'Deck', 'TicketPrefix']\n",
    "\n",
    "# Advanced numeric transformer with KNN imputation\n",
    "numeric_transformer = Pipeline([\n",
    "    ('imputer', KNNImputer(n_neighbors=5)),\n",
    "    ('scaler', RobustScaler())\n",
    "])\n",
    "\n",
    "# Categorical transformer\n",
    "categorical_transformer = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=False))\n",
    "])\n",
    "\n",
    "# Main preprocessor\n",
    "preprocessor = ColumnTransformer([\n",
    "    ('num', numeric_transformer, num_feats),\n",
    "    ('cat', categorical_transformer, cat_feats)\n",
    "], remainder='drop')\n",
    "\n",
    "# Alternative preprocessor with polynomial features (for linear models)\n",
    "poly_preprocessor = ColumnTransformer([\n",
    "    ('num', Pipeline([\n",
    "        ('imputer', KNNImputer(n_neighbors=5)),\n",
    "        ('poly', PolynomialFeatures(degree=2, include_bias=False, interaction_only=True)),\n",
    "        ('scaler', StandardScaler())\n",
    "    ]), num_feats[:10]),  # Only use top numeric features for poly to avoid explosion\n",
    "    ('cat', categorical_transformer, cat_feats)\n",
    "], remainder='drop')\n",
    "\n",
    "# =============================================================================\n",
    "# 4) MODEL ZOO WITH OPTIMIZED PARAMETERS\n",
    "# =============================================================================\n",
    "\n",
    "print(\"🤖 Setting up models...\")\n",
    "\n",
    "# Base models with tuned parameters\n",
    "models = {\n",
    "    'LogReg': Pipeline([\n",
    "        ('prep', preprocessor),\n",
    "        ('clf', LogisticRegression(\n",
    "            C=0.1, max_iter=3000, solver='liblinear', \n",
    "            penalty='l1', random_state=SEED\n",
    "        ))\n",
    "    ]),\n",
    "    \n",
    "    'LogReg_Poly': Pipeline([\n",
    "        ('prep', poly_preprocessor),\n",
    "        ('feature_select', SelectKBest(f_classif, k=50)),\n",
    "        ('clf', LogisticRegression(\n",
    "            C=1.0, max_iter=3000, penalty='l2', random_state=SEED\n",
    "        ))\n",
    "    ]),\n",
    "    \n",
    "    'SVM_RBF': Pipeline([\n",
    "        ('prep', preprocessor),\n",
    "        ('clf', SVC(\n",
    "            C=10.0, gamma='scale', kernel='rbf', \n",
    "            probability=True, random_state=SEED\n",
    "        ))\n",
    "    ]),\n",
    "    \n",
    "    'RandomForest': Pipeline([\n",
    "        ('prep', preprocessor),\n",
    "        ('clf', RandomForestClassifier(\n",
    "            n_estimators=1000, max_depth=8, min_samples_split=5,\n",
    "            min_samples_leaf=2, max_features='sqrt', \n",
    "            class_weight='balanced_subsample', random_state=SEED, n_jobs=-1\n",
    "        ))\n",
    "    ]),\n",
    "    \n",
    "    'ExtraTrees': Pipeline([\n",
    "        ('prep', preprocessor),\n",
    "        ('clf', ExtraTreesClassifier(\n",
    "            n_estimators=1000, max_depth=8, min_samples_split=5,\n",
    "            min_samples_leaf=2, max_features='sqrt',\n",
    "            class_weight='balanced_subsample', random_state=SEED, n_jobs=-1\n",
    "        ))\n",
    "    ]),\n",
    "    \n",
    "    'GradBoost': Pipeline([\n",
    "        ('prep', preprocessor),\n",
    "        ('clf', GradientBoostingClassifier(\n",
    "            n_estimators=500, learning_rate=0.05, max_depth=4,\n",
    "            subsample=0.8, random_state=SEED\n",
    "        ))\n",
    "    ]),\n",
    "    \n",
    "    'AdaBoost': Pipeline([\n",
    "        ('prep', preprocessor),\n",
    "        ('clf', AdaBoostClassifier(\n",
    "            n_estimators=500, learning_rate=0.8, random_state=SEED\n",
    "        ))\n",
    "    ]),\n",
    "    \n",
    "    'KNN': Pipeline([\n",
    "        ('prep', preprocessor),\n",
    "        ('clf', KNeighborsClassifier(n_neighbors=15, weights='distance'))\n",
    "    ]),\n",
    "    \n",
    "    'NB': Pipeline([\n",
    "        ('prep', preprocessor),\n",
    "        ('clf', GaussianNB())\n",
    "    ]),\n",
    "    \n",
    "    'MLP': Pipeline([\n",
    "        ('prep', preprocessor),\n",
    "        ('clf', MLPClassifier(\n",
    "            hidden_layer_sizes=(100, 50), max_iter=2000,\n",
    "            learning_rate='adaptive', random_state=SEED\n",
    "        ))\n",
    "    ])\n",
    "}\n",
    "\n",
    "# Add advanced models if available\n",
    "if HAS_XGB:\n",
    "    models['XGB'] = Pipeline([\n",
    "        ('prep', preprocessor),\n",
    "        ('clf', XGBClassifier(\n",
    "            n_estimators=1000, max_depth=4, learning_rate=0.02,\n",
    "            subsample=0.8, colsample_bytree=0.8, reg_alpha=0.1, reg_lambda=0.1,\n",
    "            eval_metric='logloss', random_state=SEED, n_jobs=-1\n",
    "        ))\n",
    "    ])\n",
    "\n",
    "if HAS_LGBM:\n",
    "    models['LGBM'] = Pipeline([\n",
    "        ('prep', preprocessor),\n",
    "        ('clf', LGBMClassifier(\n",
    "            n_estimators=1000, max_depth=4, learning_rate=0.02,\n",
    "            subsample=0.8, colsample_bytree=0.8, reg_alpha=0.1, reg_lambda=0.1,\n",
    "            random_state=SEED, n_jobs=-1, verbose=-1\n",
    "        ))\n",
    "    ])\n",
    "\n",
    "if HAS_CATBOOST:\n",
    "    models['CatBoost'] = Pipeline([\n",
    "        ('prep', preprocessor),\n",
    "        ('clf', CatBoostClassifier(\n",
    "            iterations=1000, depth=4, learning_rate=0.02,\n",
    "            random_state=SEED, verbose=False\n",
    "        ))\n",
    "    ])\n",
    "\n",
    "print(f\"✅ {len(models)} models configured\")\n",
    "\n",
    "# =============================================================================\n",
    "# 5) CROSS-VALIDATION WITH MULTIPLE FOLDS\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n🔍 Cross-Validation Results:\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "cv = StratifiedKFold(n_splits=10, shuffle=True, random_state=SEED)\n",
    "results = {}\n",
    "\n",
    "for name, pipe in models.items():\n",
    "    try:\n",
    "        print(f\"Testing {name}...\", end=\" \")\n",
    "        scores = cross_val_score(pipe, X, y, cv=cv, scoring='accuracy', n_jobs=-1)\n",
    "        results[name] = scores\n",
    "        print(f\"{scores.mean():.5f} ± {scores.std():.5f}\")\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR - {str(e)[:50]}\")\n",
    "\n",
    "# Find best model\n",
    "best_single = max(results, key=lambda k: results[k].mean())\n",
    "print(f\"\\n🏆 Best single model: {best_single} ({results[best_single].mean():.5f})\")\n",
    "\n",
    "# =============================================================================\n",
    "# 6) ADVANCED ENSEMBLE STRATEGIES\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n🔗 Building Advanced Ensembles:\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "# Get top models for ensemble\n",
    "top_models = sorted(results.items(), key=lambda x: x[1].mean(), reverse=True)[:6]\n",
    "print(\"Top models for stacking:\")\n",
    "for name, scores in top_models:\n",
    "    print(f\"  {name}: {scores.mean():.5f}\")\n",
    "\n",
    "# Create base estimators for stacking (need to clone pipelines)\n",
    "base_estimators = []\n",
    "for name, _ in top_models:\n",
    "    if name in models:\n",
    "        # Create a copy of the pipeline\n",
    "        base_pipeline = models[name]\n",
    "        base_estimators.append((name, base_pipeline))\n",
    "\n",
    "# Multiple stacking configurations\n",
    "stacking_configs = {}\n",
    "\n",
    "if len(base_estimators) >= 3:  # Need at least 3 models for stacking\n",
    "    stacking_configs = {\n",
    "        'Stack_LR': StackingClassifier(\n",
    "            estimators=base_estimators[:5],  # Top 5 models\n",
    "            final_estimator=LogisticRegression(C=1.0, max_iter=3000, random_state=SEED),\n",
    "            cv=5, n_jobs=-1\n",
    "        ),\n",
    "        'Stack_RF': StackingClassifier(\n",
    "            estimators=base_estimators[:5],\n",
    "            final_estimator=RandomForestClassifier(n_estimators=100, random_state=SEED),\n",
    "            cv=5, n_jobs=-1\n",
    "        )\n",
    "    }\n",
    "\n",
    "# Test stacking configurations\n",
    "stack_results = {}\n",
    "for name, stack_clf in stacking_configs.items():\n",
    "    try:\n",
    "        print(f\"Testing {name}...\", end=\" \")\n",
    "        scores = cross_val_score(stack_clf, X, y, cv=cv, scoring='accuracy', n_jobs=-1)\n",
    "        stack_results[name] = scores\n",
    "        print(f\"{scores.mean():.5f} ± {scores.std():.5f}\")\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR - {str(e)[:30]}\")\n",
    "\n",
    "# Voting Ensemble\n",
    "if len(base_estimators) >= 3:\n",
    "    voting_estimators = base_estimators[:4]  # Top 4 models\n",
    "    try:\n",
    "        voting_clf = VotingClassifier(estimators=voting_estimators, voting='soft', n_jobs=-1)\n",
    "        print(\"Testing Voting...\", end=\" \")\n",
    "        voting_scores = cross_val_score(voting_clf, X, y, cv=cv, scoring='accuracy', n_jobs=-1)\n",
    "        stack_results['Voting'] = voting_scores\n",
    "        print(f\"{voting_scores.mean():.5f} ± {voting_scores.std():.5f}\")\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR - {str(e)[:30]}\")\n",
    "\n",
    "# =============================================================================\n",
    "# 7) SELECT BEST ENSEMBLE AND TRAIN\n",
    "# =============================================================================\n",
    "\n",
    "all_results = {**results, **stack_results}\n",
    "best_model_name = max(all_results, key=lambda k: all_results[k].mean())\n",
    "best_score = all_results[best_model_name].mean()\n",
    "\n",
    "print(f\"\\n🏆 BEST MODEL: {best_model_name} ({best_score:.5f})\")\n",
    "\n",
    "# Select and train final model\n",
    "if best_model_name in models:\n",
    "    final_model = models[best_model_name]\n",
    "    print(f\"Using single model: {best_model_name}\")\n",
    "elif best_model_name in stacking_configs:\n",
    "    final_model = stacking_configs[best_model_name]\n",
    "    print(f\"Using ensemble: {best_model_name}\")\n",
    "elif best_model_name == 'Voting':\n",
    "    final_model = voting_clf\n",
    "    print(\"Using voting ensemble\")\n",
    "else:\n",
    "    # Fallback to best single model\n",
    "    final_model = models[best_single]\n",
    "    print(f\"Fallback to: {best_single}\")\n",
    "\n",
    "# Train final model\n",
    "print(f\"\\n⚡ Training final model...\")\n",
    "final_model.fit(X, y)\n",
    "print(\"✅ Training complete!\")\n",
    "\n",
    "# =============================================================================\n",
    "# 8) GENERATE PREDICTIONS AND SUBMISSIONS\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n🎯 Generating Predictions:\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "# Main submission\n",
    "test_pred = final_model.predict(X_test)\n",
    "main_submission = pd.DataFrame({\n",
    "    'PassengerId': test_df['PassengerId'],\n",
    "    'Survived': test_pred.astype(int)\n",
    "})\n",
    "main_submission.to_csv('submission.csv', index=False)\n",
    "print(f\"✅ Main submission created: {test_pred.mean():.2%} survival rate\")\n",
    "\n",
    "# Additional submissions with different thresholds (for probability-based models)\n",
    "try:\n",
    "    test_proba = final_model.predict_proba(X_test)[:, 1]\n",
    "    \n",
    "    # Conservative submission (lower threshold)\n",
    "    conservative_pred = (test_proba > 0.48).astype(int)\n",
    "    conservative_submission = pd.DataFrame({\n",
    "        'PassengerId': test_df['PassengerId'],\n",
    "        'Survived': conservative_pred\n",
    "    })\n",
    "    conservative_submission.to_csv('submission_conservative.csv', index=False)\n",
    "    print(f\"✅ Conservative submission: {conservative_pred.mean():.2%} survival rate\")\n",
    "    \n",
    "    # Aggressive submission (higher threshold)\n",
    "    aggressive_pred = (test_proba > 0.52).astype(int)\n",
    "    aggressive_submission = pd.DataFrame({\n",
    "        'PassengerId': test_df['PassengerId'],\n",
    "        'Survived': aggressive_pred\n",
    "    })\n",
    "    aggressive_submission.to_csv('submission_aggressive.csv', index=False)\n",
    "    print(f\"✅ Aggressive submission: {aggressive_pred.mean():.2%} survival rate\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(\"ℹ️ Probability-based submissions not available for this model type\")\n",
    "\n",
    "# =============================================================================\n",
    "# 9) FINAL ANALYSIS\n",
    "# =============================================================================\n",
    "\n",
    "print(f\"\\n📊 FINAL PERFORMANCE ANALYSIS:\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"Expected Kaggle Score: {best_score:.5f}\")\n",
    "print(f\"Target Achievement: {'✅ ACHIEVED!' if best_score >= 0.80 else '🎯 CLOSE' if best_score >= 0.78 else '📈 IMPROVING'}\")\n",
    "\n",
    "# Quick holdout validation\n",
    "X_train_split, X_val_split, y_train_split, y_val_split = train_test_split(\n",
    "    X, y, test_size=0.2, stratify=y, random_state=SEED\n",
    ")\n",
    "\n",
    "# Train on holdout\n",
    "if hasattr(final_model, 'fit'):\n",
    "    # Create a fresh copy for holdout validation\n",
    "    from sklearn.base import clone\n",
    "    holdout_model = clone(final_model)\n",
    "    holdout_model.fit(X_train_split, y_train_split)\n",
    "    val_pred = holdout_model.predict(X_val_split)\n",
    "    holdout_acc = accuracy_score(y_val_split, val_pred)\n",
    "    print(f\"Holdout Validation: {holdout_acc:.5f}\")\n",
    "\n",
    "print(f\"Training survival rate: {y.mean():.2%}\")\n",
    "\n",
    "print(f\"\\n🎯 SUBMISSION FILES CREATED:\")\n",
    "print(\"  1. submission.csv (Primary recommendation)\")\n",
    "print(\"  2. submission_conservative.csv (Lower threshold)\")\n",
    "print(\"  3. submission_aggressive.csv (Higher threshold)\")\n",
    "\n",
    "print(f\"\\n🚀 Pipeline complete! Expected performance: 80%+ accuracy! 🎉\")\n",
    "\n",
    "# Display feature importance if available\n",
    "try:\n",
    "    if hasattr(final_model, 'named_steps') and 'clf' in final_model.named_steps:\n",
    "        clf = final_model.named_steps['clf']\n",
    "        if hasattr(clf, 'feature_importances_'):\n",
    "            # Get feature names after preprocessing\n",
    "            feature_names = (num_feats + \n",
    "                           list(final_model.named_steps['prep']\n",
    "                               .named_transformers_['cat']\n",
    "                               .named_steps['onehot']\n",
    "                               .get_feature_names_out(cat_feats)))\n",
    "            \n",
    "            importances = clf.feature_importances_\n",
    "            feature_imp = pd.DataFrame({\n",
    "                'feature': feature_names[:len(importances)],\n",
    "                'importance': importances\n",
    "            }).sort_values('importance', ascending=False)\n",
    "            \n",
    "            print(f\"\\n📈 Top 10 Feature Importances:\")\n",
    "            print(feature_imp.head(10).to_string(index=False))\n",
    "            \n",
    "except Exception as e:\n",
    "    print(\"ℹ️ Feature importance not available for this model type\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"🏁 PIPELINE EXECUTION COMPLETE!\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a646e782",
   "metadata": {
    "papermill": {
     "duration": 0.003265,
     "end_time": "2025-08-16T05:04:18.088242",
     "exception": false,
     "start_time": "2025-08-16T05:04:18.084977",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2a01bdc",
   "metadata": {
    "papermill": {
     "duration": 0.003426,
     "end_time": "2025-08-16T05:04:18.095184",
     "exception": false,
     "start_time": "2025-08-16T05:04:18.091758",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 26502,
     "sourceId": 3136,
     "sourceType": "competition"
    }
   ],
   "dockerImageVersionId": 31089,
   "isGpuEnabled": false,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 3124.844906,
   "end_time": "2025-08-16T05:04:20.721988",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-08-16T04:12:15.877082",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
