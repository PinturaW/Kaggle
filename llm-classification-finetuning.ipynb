{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6daab246",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-08-14T11:41:26.965448Z",
     "iopub.status.busy": "2025-08-14T11:41:26.965125Z",
     "iopub.status.idle": "2025-08-14T11:41:28.895551Z",
     "shell.execute_reply": "2025-08-14T11:41:28.894538Z"
    },
    "papermill": {
     "duration": 1.935837,
     "end_time": "2025-08-14T11:41:28.897136",
     "exception": false,
     "start_time": "2025-08-14T11:41:26.961299",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/llm-classification-finetuning/sample_submission.csv\n",
      "/kaggle/input/llm-classification-finetuning/train.csv\n",
      "/kaggle/input/llm-classification-finetuning/test.csv\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "40a28e1a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-14T11:41:28.903549Z",
     "iopub.status.busy": "2025-08-14T11:41:28.903114Z",
     "iopub.status.idle": "2025-08-14T14:56:01.154141Z",
     "shell.execute_reply": "2025-08-14T14:56:01.151809Z"
    },
    "papermill": {
     "duration": 11672.269098,
     "end_time": "2025-08-14T14:56:01.168566",
     "exception": false,
     "start_time": "2025-08-14T11:41:28.899468",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes: (57477, 9) (3, 4)\n",
      "Engineering features (train)…\n",
      "Engineering features (test)…\n",
      "Combined features: (57477, 6046) (3, 6046)\n",
      "LR logloss: 1.0895\n",
      "XGB logloss: 1.0371\n",
      "LGB logloss: 1.0379\n",
      "RF logloss: 1.0516\n",
      "Model weights (LR, XGB, LGB, RF): [0.242 0.254 0.254 0.25 ]\n",
      "Ensemble logloss: 1.0383\n",
      "Fold 1 logloss: 1.0421\n",
      "Fold 2 logloss: 1.0423\n",
      "Fold 3 logloss: 1.0389\n",
      "Fold 4 logloss: 1.0418\n",
      "Fold 5 logloss: 1.0446\n",
      "CV mean ± 2*std: 1.0419416205513516 ± 0.0036310502623490225\n",
      "Saved submission.csv\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# LLM Classification Finetuning — Advanced Ensemble (No textstat)\n",
    "# Goal: strong baseline with rich features + LR/XGBoost/LightGBM/RF ensemble\n",
    "# Produces: submission.csv\n",
    "# ============================================================\n",
    "\n",
    "import re, warnings, numpy as np, pandas as pd\n",
    "from pathlib import Path\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import log_loss\n",
    "from scipy.sparse import hstack, csr_matrix\n",
    "\n",
    "# XGBoost / LightGBM are preinstalled in Kaggle\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "DATA_DIR = Path(\"/kaggle/input/llm-classification-finetuning\")\n",
    "assert DATA_DIR.exists(), \"Add competition dataset on the right panel.\"\n",
    "\n",
    "# ------------------------ Load ------------------------\n",
    "train_df = pd.read_csv(DATA_DIR / \"train.csv\")\n",
    "test_df  = pd.read_csv(DATA_DIR / \"test.csv\")\n",
    "target_cols = ['winner_model_a', 'winner_model_b', 'winner_tie']\n",
    "\n",
    "print(\"Shapes:\", train_df.shape, test_df.shape)\n",
    "\n",
    "# ------------------------ Targets ------------------------\n",
    "def get_winner_label(row):\n",
    "    if row['winner_model_a'] == 1: return 0\n",
    "    if row['winner_model_b'] == 1: return 1\n",
    "    return 2\n",
    "\n",
    "train_df['winner_label'] = train_df.apply(get_winner_label, axis=1)\n",
    "y = train_df['winner_label'].values\n",
    "y_multiclass = train_df[target_cols].values\n",
    "\n",
    "# ------------------------ Utilities ------------------------\n",
    "def safe_len_words(s):\n",
    "    if not isinstance(s, str) or not s: return 0\n",
    "    return len(s.split())\n",
    "\n",
    "def safe_len_chars(s):\n",
    "    if not isinstance(s, str): return 0\n",
    "    return len(s)\n",
    "\n",
    "def split_sentences(s):\n",
    "    if not isinstance(s, str) or not s: return []\n",
    "    # split on ., !, ? while keeping it simple\n",
    "    parts = re.split(r'[.!?]+', s)\n",
    "    return [p.strip() for p in parts if p.strip()]\n",
    "\n",
    "def estimate_syllables_word(word: str) -> int:\n",
    "    \"\"\"\n",
    "    Very rough English syllable estimator (vowel groups).\n",
    "    For non-Latin scripts, fall back to 1.\n",
    "    \"\"\"\n",
    "    if not word:\n",
    "        return 0\n",
    "    if not re.search(r'[a-zA-Z]', word):\n",
    "        return 1\n",
    "    w = word.lower()\n",
    "    w = re.sub(r'[^a-z]', '', w)\n",
    "    if not w:\n",
    "        return 1\n",
    "    groups = re.findall(r'[aeiouy]+', w)\n",
    "    count = len(groups)\n",
    "    # silent 'e'\n",
    "    if w.endswith('e') and len(groups) > 1:\n",
    "        count -= 1\n",
    "    return max(1, count)\n",
    "\n",
    "def estimate_syllables_text(text: str) -> int:\n",
    "    if not isinstance(text, str) or not text:\n",
    "        return 0\n",
    "    words = text.split()\n",
    "    return int(sum(estimate_syllables_word(w) for w in words))\n",
    "\n",
    "def flesch_reading_ease_proxy(text: str) -> float:\n",
    "    \"\"\"\n",
    "    Flesch Reading Ease (proxy using our syllable estimator):\n",
    "    206.835 - 1.015*(words/sentences) - 84.6*(syllables/words)\n",
    "    Handles edge cases safely. Returns value clipped to [-50, 120].\n",
    "    \"\"\"\n",
    "    if not isinstance(text, str) or not text.strip():\n",
    "        return 0.0\n",
    "    words = max(1, safe_len_words(text))\n",
    "    sents = max(1, len(split_sentences(text)))\n",
    "    sylls = max(1, estimate_syllables_text(text))\n",
    "    fre = 206.835 - 1.015*(words/sents) - 84.6*(sylls/words)\n",
    "    return float(np.clip(fre, -50, 120))\n",
    "\n",
    "def fk_grade_proxy(text: str) -> float:\n",
    "    \"\"\"\n",
    "    Flesch-Kincaid Grade Level (proxy):\n",
    "    0.39*(words/sentences) + 11.8*(syllables/words) - 15.59\n",
    "    Clipped to [0, 20]\n",
    "    \"\"\"\n",
    "    if not isinstance(text, str) or not text.strip():\n",
    "        return 0.0\n",
    "    words = max(1, safe_len_words(text))\n",
    "    sents = max(1, len(split_sentences(text)))\n",
    "    sylls = max(1, estimate_syllables_text(text))\n",
    "    grade = 0.39*(words/sents) + 11.8*(sylls/words) - 15.59\n",
    "    return float(np.clip(grade, 0, 20))\n",
    "\n",
    "# ------------------------ Feature Engineering ------------------------\n",
    "def advanced_text_features(text):\n",
    "    if not isinstance(text, str) or text == '':\n",
    "        return {\n",
    "            'char_count': 0, 'word_count': 0, 'sentence_count': 0,\n",
    "            'avg_word_length': 0, 'question_count': 0, 'exclamation_count': 0,\n",
    "            'uppercase_ratio': 0, 'digit_count': 0, 'special_char_count': 0,\n",
    "            'readability_score': 0, 'grade_level': 0\n",
    "        }\n",
    "    char_count = len(text)\n",
    "    words = text.split()\n",
    "    word_count = len(words)\n",
    "    sentences = split_sentences(text)\n",
    "    sentence_count = len(sentences) if sentences else 1\n",
    "    avg_word_length = float(np.mean([len(w) for w in words])) if words else 0.0\n",
    "    question_count = text.count('?')\n",
    "    exclamation_count = text.count('!')\n",
    "    uppercase_ratio = sum(1 for c in text if c.isupper()) / max(1, len(text))\n",
    "    digit_count = sum(1 for c in text if c.isdigit())\n",
    "    special_char_count = len(re.findall(r'[^a-zA-Z0-9\\s]', text))\n",
    "\n",
    "    # proxies (no textstat)\n",
    "    readability_score = flesch_reading_ease_proxy(text)\n",
    "    grade_level = fk_grade_proxy(text)\n",
    "\n",
    "    return {\n",
    "        'char_count': char_count,\n",
    "        'word_count': word_count,\n",
    "        'sentence_count': sentence_count,\n",
    "        'avg_word_length': avg_word_length,\n",
    "        'question_count': question_count,\n",
    "        'exclamation_count': exclamation_count,\n",
    "        'uppercase_ratio': uppercase_ratio,\n",
    "        'digit_count': digit_count,\n",
    "        'special_char_count': special_char_count,\n",
    "        'readability_score': readability_score,\n",
    "        'grade_level': grade_level\n",
    "    }\n",
    "\n",
    "def create_advanced_features(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = df.copy()\n",
    "\n",
    "    # Prompt / A / B individual features\n",
    "    prompt_feat = df['prompt'].apply(advanced_text_features)\n",
    "    for k in prompt_feat.iloc[0].keys():\n",
    "        df[f'prompt_{k}'] = [d[k] for d in prompt_feat]\n",
    "\n",
    "    a_feat = df['response_a'].apply(advanced_text_features)\n",
    "    for k in a_feat.iloc[0].keys():\n",
    "        df[f'response_a_{k}'] = [d[k] for d in a_feat]\n",
    "\n",
    "    b_feat = df['response_b'].apply(advanced_text_features)\n",
    "    for k in b_feat.iloc[0].keys():\n",
    "        df[f'response_b_{k}'] = [d[k] for d in b_feat]\n",
    "\n",
    "    # Comparative features\n",
    "    df['length_diff'] = df['response_a_char_count'] - df['response_b_char_count']\n",
    "    df['length_ratio'] = df['response_a_char_count'] / (df['response_b_char_count'] + 1)\n",
    "    df['word_diff'] = df['response_a_word_count'] - df['response_b_word_count']\n",
    "    df['word_ratio'] = df['response_a_word_count'] / (df['response_b_word_count'] + 1)\n",
    "    df['sentence_diff'] = df['response_a_sentence_count'] - df['response_b_sentence_count']\n",
    "    df['readability_diff'] = df['response_a_readability_score'] - df['response_b_readability_score']\n",
    "    df['grade_diff'] = df['response_a_grade_level'] - df['response_b_grade_level']\n",
    "\n",
    "    # Simple quality indicators\n",
    "    df['a_more_detailed'] = (df['response_a_char_count'] > df['response_b_char_count']).astype(int)\n",
    "    df['a_more_questions'] = (df['response_a_question_count'] > df['response_b_question_count']).astype(int)\n",
    "    df['a_more_readable'] = (df['response_a_readability_score'] > df['response_b_readability_score']).astype(int)\n",
    "    df['a_better_grade'] = (df['response_a_grade_level'] < df['response_b_grade_level']).astype(int)\n",
    "\n",
    "    # Interaction proxies\n",
    "    df['prompt_response_a_similarity'] = df['prompt_word_count'] / (df['response_a_word_count'] + 1)\n",
    "    df['prompt_response_b_similarity'] = df['prompt_word_count'] / (df['response_b_word_count'] + 1)\n",
    "\n",
    "    # Texts for vectorization\n",
    "    df['combined_text'] = df['prompt'].astype(str) + \" [SEP] \" + df['response_a'].astype(str) + \" [SEP] \" + df['response_b'].astype(str)\n",
    "    df['response_comparison'] = df['response_a'].astype(str) + \" [CMP] \" + df['response_b'].astype(str)\n",
    "\n",
    "    return df\n",
    "\n",
    "print(\"Engineering features (train)…\")\n",
    "train_df = create_advanced_features(train_df)\n",
    "print(\"Engineering features (test)…\")\n",
    "test_df  = create_advanced_features(test_df)\n",
    "\n",
    "# ------------------------ Numeric feature lists ------------------------\n",
    "numerical_features = [c for c in train_df.columns if any(x in c for x in [\n",
    "    'char_count', 'word_count', 'sentence_count', 'avg_word_length',\n",
    "    'question_count', 'exclamation_count', 'uppercase_ratio', 'digit_count',\n",
    "    'special_char_count', 'readability_score', 'grade_level', '_diff', '_ratio',\n",
    "    'more_detailed', 'more_questions', 'more_readable', 'better_grade', 'similarity'\n",
    "])]\n",
    "\n",
    "X_num = train_df[numerical_features].fillna(0.0).astype(np.float32)\n",
    "X_num_test = test_df[numerical_features].fillna(0.0).astype(np.float32)\n",
    "\n",
    "# Standardize numeric (dense)\n",
    "scaler = StandardScaler()\n",
    "X_num_scaled = scaler.fit_transform(X_num)\n",
    "X_num_test_scaled = scaler.transform(X_num_test)\n",
    "\n",
    "# ------------------------ TF-IDF blocks ------------------------\n",
    "def make_tfidf(train_texts, test_texts, max_features, ngram=(1,2), analyzer='word', min_df=2, max_df=0.95):\n",
    "    vec = TfidfVectorizer(max_features=max_features, ngram_range=ngram,\n",
    "                          analyzer=analyzer, min_df=min_df, max_df=max_df, sublinear_tf=True,\n",
    "                          stop_words='english')\n",
    "    Xtr = vec.fit_transform(train_texts)\n",
    "    Xte = vec.transform(test_texts)\n",
    "    return Xtr, Xte\n",
    "\n",
    "# Combined text\n",
    "X_tfidf_combined, X_tfidf_combined_test = make_tfidf(\n",
    "    train_df['combined_text'], test_df['combined_text'], max_features=3000, ngram=(1,3)\n",
    ")\n",
    "\n",
    "# Response comparison\n",
    "X_tfidf_cmp, X_tfidf_cmp_test = make_tfidf(\n",
    "    train_df['response_comparison'], test_df['response_comparison'], max_features=2000, ngram=(1,2)\n",
    ")\n",
    "\n",
    "# Char n-grams\n",
    "char_vec_tr, char_vec_te = make_tfidf(\n",
    "    train_df['combined_text'], test_df['combined_text'], max_features=1000, ngram=(2,4), analyzer='char', min_df=5, max_df=0.9\n",
    ")\n",
    "\n",
    "# Combine\n",
    "X_combined = hstack([\n",
    "    csr_matrix(X_num_scaled),\n",
    "    X_tfidf_combined,\n",
    "    X_tfidf_cmp,\n",
    "    char_vec_tr\n",
    "])\n",
    "X_combined_test = hstack([\n",
    "    csr_matrix(X_num_test_scaled),\n",
    "    X_tfidf_combined_test,\n",
    "    X_tfidf_cmp_test,\n",
    "    char_vec_te\n",
    "])\n",
    "\n",
    "print(\"Combined features:\", X_combined.shape, X_combined_test.shape)\n",
    "\n",
    "# ------------------------ Train/Val split ------------------------\n",
    "X_tr, X_val, y_tr, y_val = train_test_split(\n",
    "    X_combined, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "_, _, y_tr_multi, y_val_multi = train_test_split(\n",
    "    X_combined, y_multiclass, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# ------------------------ Models ------------------------\n",
    "# 1) Logistic Regression\n",
    "lr = LogisticRegression(multi_class='multinomial', solver='lbfgs',\n",
    "                        max_iter=2000, random_state=42, C=0.5, class_weight='balanced')\n",
    "lr.fit(X_tr, y_tr)\n",
    "lr_pred = lr.predict_proba(X_val)\n",
    "lr_loss = log_loss(y_val_multi, lr_pred)\n",
    "print(f\"LR logloss: {lr_loss:.4f}\")\n",
    "\n",
    "# 2) XGBoost\n",
    "xgb_clf = xgb.XGBClassifier(\n",
    "    n_estimators=220, max_depth=6, learning_rate=0.09,\n",
    "    subsample=0.9, colsample_bytree=0.9, random_state=42,\n",
    "    eval_metric='mlogloss', tree_method='hist'\n",
    ")\n",
    "xgb_clf.fit(X_tr, y_tr)\n",
    "xgb_pred = xgb_clf.predict_proba(X_val)\n",
    "xgb_loss = log_loss(y_val_multi, xgb_pred)\n",
    "print(f\"XGB logloss: {xgb_loss:.4f}\")\n",
    "\n",
    "# 3) LightGBM\n",
    "lgb_clf = lgb.LGBMClassifier(\n",
    "    n_estimators=300, max_depth=6, learning_rate=0.08,\n",
    "    subsample=0.9, colsample_bytree=0.9, random_state=42,\n",
    "    objective='multiclass', num_class=3, metric='multi_logloss', verbose=-1\n",
    ")\n",
    "lgb_clf.fit(X_tr, y_tr)\n",
    "lgb_pred = lgb_clf.predict_proba(X_val)\n",
    "lgb_loss = log_loss(y_val_multi, lgb_pred)\n",
    "print(f\"LGB logloss: {lgb_loss:.4f}\")\n",
    "\n",
    "# 4) Random Forest\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=180, max_depth=12, min_samples_split=5, min_samples_leaf=2,\n",
    "    class_weight='balanced', n_jobs=-1, random_state=42\n",
    ")\n",
    "rf.fit(X_tr, y_tr)\n",
    "rf_pred = rf.predict_proba(X_val)\n",
    "rf_loss = log_loss(y_val_multi, rf_pred)\n",
    "print(f\"RF logloss: {rf_loss:.4f}\")\n",
    "\n",
    "# Weighted ensemble (inverse-loss weights)\n",
    "losses = np.array([lr_loss, xgb_loss, lgb_loss, rf_loss])\n",
    "weights = 1.0 / np.maximum(1e-6, losses)\n",
    "weights = weights / weights.sum()\n",
    "print(\"Model weights (LR, XGB, LGB, RF):\", np.round(weights, 3))\n",
    "\n",
    "ens_val = np.average([lr_pred, xgb_pred, lgb_pred, rf_pred], axis=0, weights=weights)\n",
    "ens_loss = log_loss(y_val_multi, ens_val)\n",
    "print(f\"Ensemble logloss: {ens_loss:.4f}\")\n",
    "\n",
    "# ------------------------ 5-fold CV on whole training ------------------------\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "cv_scores = []\n",
    "for fold, (tr_idx, vl_idx) in enumerate(skf.split(X_combined, y), 1):\n",
    "    Xtr, Xvl = X_combined[tr_idx], X_combined[vl_idx]\n",
    "    ytr, yvl = y[tr_idx], y[vl_idx]\n",
    "    yvl_multi = y_multiclass[vl_idx]\n",
    "\n",
    "    m1 = LogisticRegression(multi_class='multinomial', solver='lbfgs',\n",
    "                            max_iter=2000, random_state=42, C=0.5, class_weight='balanced').fit(Xtr, ytr)\n",
    "    m2 = xgb.XGBClassifier(\n",
    "        n_estimators=220, max_depth=6, learning_rate=0.09,\n",
    "        subsample=0.9, colsample_bytree=0.9, random_state=42,\n",
    "        eval_metric='mlogloss', tree_method='hist'\n",
    "    ).fit(Xtr, ytr)\n",
    "    m3 = lgb.LGBMClassifier(\n",
    "        n_estimators=300, max_depth=6, learning_rate=0.08,\n",
    "        subsample=0.9, colsample_bytree=0.9, random_state=42,\n",
    "        objective='multiclass', num_class=3, verbose=-1\n",
    "    ).fit(Xtr, ytr)\n",
    "    m4 = RandomForestClassifier(\n",
    "        n_estimators=180, max_depth=12, min_samples_split=5, min_samples_leaf=2,\n",
    "        class_weight='balanced', n_jobs=-1, random_state=42\n",
    "    ).fit(Xtr, ytr)\n",
    "\n",
    "    preds = [m1.predict_proba(Xvl), m2.predict_proba(Xvl), m3.predict_proba(Xvl), m4.predict_proba(Xvl)]\n",
    "    fold_pred = np.average(preds, axis=0, weights=weights)\n",
    "    fold_loss = log_loss(yvl_multi, fold_pred)\n",
    "    cv_scores.append(fold_loss)\n",
    "    print(f\"Fold {fold} logloss: {fold_loss:.4f}\")\n",
    "\n",
    "print(\"CV mean ± 2*std:\", np.mean(cv_scores), \"±\", 2*np.std(cv_scores))\n",
    "\n",
    "# ------------------------ Final fit on full data ------------------------\n",
    "final_lr  = LogisticRegression(multi_class='multinomial', solver='lbfgs',\n",
    "                              max_iter=2000, random_state=42, C=0.5, class_weight='balanced').fit(X_combined, y)\n",
    "final_xgb = xgb.XGBClassifier(\n",
    "    n_estimators=220, max_depth=6, learning_rate=0.09,\n",
    "    subsample=0.9, colsample_bytree=0.9, random_state=42,\n",
    "    eval_metric='mlogloss', tree_method='hist'\n",
    ").fit(X_combined, y)\n",
    "final_lgb = lgb.LGBMClassifier(\n",
    "    n_estimators=300, max_depth=6, learning_rate=0.08,\n",
    "    subsample=0.9, colsample_bytree=0.9, random_state=42,\n",
    "    objective='multiclass', num_class=3, verbose=-1\n",
    ").fit(X_combined, y)\n",
    "final_rf  = RandomForestClassifier(\n",
    "    n_estimators=180, max_depth=12, min_samples_split=5, min_samples_leaf=2,\n",
    "    class_weight='balanced', n_jobs=-1, random_state=42\n",
    ").fit(X_combined, y)\n",
    "\n",
    "# ------------------------ Predict test & save ------------------------\n",
    "preds_test = np.average(\n",
    "    [\n",
    "        final_lr.predict_proba(X_combined_test),\n",
    "        final_xgb.predict_proba(X_combined_test),\n",
    "        final_lgb.predict_proba(X_combined_test),\n",
    "        final_rf.predict_proba(X_combined_test),\n",
    "    ],\n",
    "    axis=0, weights=weights\n",
    ")\n",
    "\n",
    "submission = pd.DataFrame({\n",
    "    'id': test_df['id'],\n",
    "    'winner_model_a': preds_test[:, 0],\n",
    "    'winner_model_b': preds_test[:, 1],\n",
    "    'winner_tie':     preds_test[:, 2],\n",
    "})\n",
    "\n",
    "# small renorm (just in case)\n",
    "row_sum = submission[['winner_model_a','winner_model_b','winner_tie']].sum(axis=1)\n",
    "for c in ['winner_model_a','winner_model_b','winner_tie']:\n",
    "    submission[c] = submission[c] / row_sum\n",
    "\n",
    "submission.to_csv(\"submission.csv\", index=False)\n",
    "print(\"Saved submission.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0392ce7",
   "metadata": {
    "papermill": {
     "duration": 0.003782,
     "end_time": "2025-08-14T14:56:01.177548",
     "exception": false,
     "start_time": "2025-08-14T14:56:01.173766",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 9809560,
     "sourceId": 86518,
     "sourceType": "competition"
    }
   ],
   "dockerImageVersionId": 31089,
   "isGpuEnabled": false,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 11682.306414,
   "end_time": "2025-08-14T14:56:04.247700",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-08-14T11:41:21.941286",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
