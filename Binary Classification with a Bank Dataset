{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":91719,"databundleVersionId":12937777,"sourceType":"competition"}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Binary Classification with a Bank Dataset - Kaggle Competition\n# Playground Series Season 5, Episode 8\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold\nfrom sklearn.preprocessing import StandardScaler, LabelEncoder\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import roc_auc_score, classification_report, confusion_matrix\nfrom sklearn.impute import SimpleImputer\nimport lightgbm as lgb\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Set style for plots\nplt.style.use('seaborn-v0_8')\nsns.set_palette(\"husl\")\n\nprint(\"üìä Binary Classification with Bank Dataset\")\nprint(\"=\" * 50)\n\n# Load the data\nprint(\"üîÑ Loading data...\")\ntrain_df = pd.read_csv('/kaggle/input/playground-series-s5e8/train.csv')\ntest_df = pd.read_csv('/kaggle/input/playground-series-s5e8/test.csv')\nsample_submission = pd.read_csv('/kaggle/input/playground-series-s5e8/sample_submission.csv')\n\nprint(f\"‚úÖ Train data shape: {train_df.shape}\")\nprint(f\"‚úÖ Test data shape: {test_df.shape}\")\nprint(f\"‚úÖ Sample submission shape: {sample_submission.shape}\")\n\n# ==============================================================================\n# EXPLORATORY DATA ANALYSIS\n# ==============================================================================\n\nprint(\"\\nüìà EXPLORATORY DATA ANALYSIS\")\nprint(\"=\" * 50)\n\n# Basic info about the dataset\nprint(\"üîç Dataset Info:\")\nprint(train_df.info())\nprint(\"\\nüìä Target variable distribution:\")\nprint(train_df['y'].value_counts(normalize=True))\n\n# Check for missing values\nprint(\"\\n‚ùì Missing values in train data:\")\nmissing_train = train_df.isnull().sum()\nprint(missing_train[missing_train > 0])\n\nprint(\"\\n‚ùì Missing values in test data:\")\nmissing_test = test_df.isnull().sum()\nprint(missing_test[missing_test > 0])\n\n# Display first few rows\nprint(\"\\nüëÄ First 5 rows of training data:\")\nprint(train_df.head())\n\n# Statistical summary\nprint(\"\\nüìä Statistical summary:\")\nprint(train_df.describe())\n\n# Visualizations\nfig, axes = plt.subplots(2, 2, figsize=(15, 10))\n\n# Target distribution\naxes[0,0].pie(train_df['y'].value_counts(), labels=['No (0)', 'Yes (1)'], autopct='%1.1f%%')\naxes[0,0].set_title('Target Variable Distribution')\n\n# Correlation heatmap for numeric columns\nnumeric_cols = train_df.select_dtypes(include=[np.number]).columns\ncorrelation_matrix = train_df[numeric_cols].corr()\nsns.heatmap(correlation_matrix, annot=False, cmap='coolwarm', center=0, ax=axes[0,1])\naxes[0,1].set_title('Correlation Heatmap')\n\n# Target vs numeric features (sample of first few)\nnumeric_features = train_df.select_dtypes(include=[np.number]).columns.drop(['id', 'y'])\nif len(numeric_features) > 0:\n    feature_to_plot = numeric_features[0]  # Plot first numeric feature\n    for i, target_val in enumerate([0, 1]):\n        data = train_df[train_df['y'] == target_val][feature_to_plot].dropna()\n        axes[1,0].hist(data, alpha=0.7, label=f'y={target_val}', bins=30)\n    axes[1,0].set_title(f'{feature_to_plot} Distribution by Target')\n    axes[1,0].legend()\n\n# Feature importance preview (using a quick random forest)\nX_sample = train_df.drop(['id', 'y'], axis=1)\n# Handle categorical variables for quick analysis\nle = LabelEncoder()\nfor col in X_sample.columns:\n    if X_sample[col].dtype == 'object':\n        X_sample[col] = le.fit_transform(X_sample[col].astype(str))\n\n# Fill any remaining missing values\nX_sample = X_sample.fillna(X_sample.median())\n\nrf_quick = RandomForestClassifier(n_estimators=50, random_state=42)\nrf_quick.fit(X_sample, train_df['y'])\nfeature_importance = pd.DataFrame({\n    'feature': X_sample.columns,\n    'importance': rf_quick.feature_importances_\n}).sort_values('importance', ascending=True).tail(10)\n\naxes[1,1].barh(feature_importance['feature'], feature_importance['importance'])\naxes[1,1].set_title('Top 10 Feature Importance (Quick RF)')\n\nplt.tight_layout()\nplt.show()\n\n# ==============================================================================\n# DATA PREPROCESSING\n# ==============================================================================\n\nprint(\"\\nüîß DATA PREPROCESSING\")\nprint(\"=\" * 50)\n\ndef preprocess_data(df, is_train=True):\n    \"\"\"Preprocess the data for training/testing\"\"\"\n    df_processed = df.copy()\n    \n    # Separate ID column\n    if 'id' in df_processed.columns:\n        ids = df_processed['id']\n        df_processed = df_processed.drop('id', axis=1)\n    else:\n        ids = None\n    \n    # Separate target if training data\n    if is_train and 'y' in df_processed.columns:\n        target = df_processed['y']\n        df_processed = df_processed.drop('y', axis=1)\n    else:\n        target = None\n    \n    # Identify categorical and numerical columns\n    categorical_cols = df_processed.select_dtypes(include=['object']).columns.tolist()\n    numerical_cols = df_processed.select_dtypes(include=[np.number]).columns.tolist()\n    \n    print(f\"üìã Categorical columns: {len(categorical_cols)}\")\n    print(f\"üìã Numerical columns: {len(numerical_cols)}\")\n    \n    # Handle categorical variables\n    label_encoders = {}\n    for col in categorical_cols:\n        le = LabelEncoder()\n        df_processed[col] = le.fit_transform(df_processed[col].astype(str))\n        label_encoders[col] = le\n    \n    # Handle missing values\n    if df_processed.isnull().sum().sum() > 0:\n        # Impute numerical columns with median\n        num_imputer = SimpleImputer(strategy='median')\n        if numerical_cols:\n            df_processed[numerical_cols] = num_imputer.fit_transform(df_processed[numerical_cols])\n        \n        # Impute categorical columns with mode\n        cat_imputer = SimpleImputer(strategy='most_frequent')\n        if categorical_cols:\n            df_processed[categorical_cols] = cat_imputer.fit_transform(df_processed[categorical_cols])\n    \n    return df_processed, target, ids, label_encoders\n\n# Preprocess training data\nX_train_processed, y_train, train_ids, label_encoders = preprocess_data(train_df, is_train=True)\n\n# Preprocess test data\nX_test_processed, _, test_ids, _ = preprocess_data(test_df, is_train=False)\n\nprint(f\"‚úÖ Training features shape: {X_train_processed.shape}\")\nprint(f\"‚úÖ Test features shape: {X_test_processed.shape}\")\n\n# Feature scaling\nscaler = StandardScaler()\nX_train_scaled = scaler.fit_transform(X_train_processed)\nX_test_scaled = scaler.transform(X_test_processed)\n\n# Convert back to DataFrame for easier handling\nX_train_scaled = pd.DataFrame(X_train_scaled, columns=X_train_processed.columns)\nX_test_scaled = pd.DataFrame(X_test_scaled, columns=X_test_processed.columns)\n\n# ==============================================================================\n# MODEL TRAINING AND EVALUATION\n# ==============================================================================\n\nprint(\"\\nü§ñ MODEL TRAINING AND EVALUATION\")\nprint(\"=\" * 50)\n\n# Split training data for validation\nX_train_split, X_val_split, y_train_split, y_val_split = train_test_split(\n    X_train_scaled, y_train, test_size=0.2, random_state=42, stratify=y_train\n)\n\n# Initialize models\nmodels = {\n    'Logistic Regression': LogisticRegression(random_state=42, max_iter=1000),\n    'Random Forest': RandomForestClassifier(n_estimators=100, random_state=42),\n    'Gradient Boosting': GradientBoostingClassifier(n_estimators=100, random_state=42),\n    'LightGBM': lgb.LGBMClassifier(random_state=42, verbosity=-1)\n}\n\n# Train and evaluate models\nmodel_scores = {}\ntrained_models = {}\n\nprint(\"üèÅ Training models...\")\nfor name, model in models.items():\n    print(f\"\\nüîÑ Training {name}...\")\n    \n    # Train model\n    model.fit(X_train_split, y_train_split)\n    \n    # Predict probabilities\n    y_pred_proba = model.predict_proba(X_val_split)[:, 1]\n    \n    # Calculate ROC AUC\n    roc_auc = roc_auc_score(y_val_split, y_pred_proba)\n    model_scores[name] = roc_auc\n    trained_models[name] = model\n    \n    print(f\"‚úÖ {name} ROC AUC: {roc_auc:.4f}\")\n\n# Cross-validation for best model\nbest_model_name = max(model_scores.keys(), key=lambda k: model_scores[k])\nbest_model = trained_models[best_model_name]\n\nprint(f\"\\nüèÜ Best model: {best_model_name} (ROC AUC: {model_scores[best_model_name]:.4f})\")\n\n# Perform cross-validation on the best model\ncv_scores = cross_val_score(\n    best_model, X_train_scaled, y_train, \n    cv=StratifiedKFold(n_splits=5, shuffle=True, random_state=42), \n    scoring='roc_auc'\n)\n\nprint(f\"üîÑ 5-Fold Cross-Validation ROC AUC: {cv_scores.mean():.4f} ¬± {cv_scores.std():.4f}\")\n\n# ==============================================================================\n# ENSEMBLE APPROACH\n# ==============================================================================\n\nprint(\"\\nüéØ ENSEMBLE APPROACH\")\nprint(\"=\" * 50)\n\n# Create ensemble predictions\nensemble_pred_val = np.zeros(len(X_val_split))\nensemble_pred_test = np.zeros(len(X_test_scaled))\n\n# Weight models based on their performance\ntotal_score = sum(model_scores.values())\nweights = {name: score/total_score for name, score in model_scores.items()}\n\nprint(\"üìä Model weights:\")\nfor name, weight in weights.items():\n    print(f\"   {name}: {weight:.3f}\")\n\n# Generate ensemble predictions\nfor name, model in trained_models.items():\n    # Retrain on full training data\n    model.fit(X_train_scaled, y_train)\n    \n    # Validation predictions (for ensemble evaluation)\n    val_pred = model.predict_proba(X_val_split)[:, 1]\n    ensemble_pred_val += weights[name] * val_pred\n    \n    # Test predictions\n    test_pred = model.predict_proba(X_test_scaled)[:, 1]\n    ensemble_pred_test += weights[name] * test_pred\n\n# Evaluate ensemble\nensemble_roc_auc = roc_auc_score(y_val_split, ensemble_pred_val)\nprint(f\"üèÜ Ensemble ROC AUC: {ensemble_roc_auc:.4f}\")\n\n# ==============================================================================\n# FINAL PREDICTIONS AND SUBMISSION\n# ==============================================================================\n\nprint(\"\\nüìù GENERATING FINAL PREDICTIONS\")\nprint(\"=\" * 50)\n\n# Create submission file\nsubmission = pd.DataFrame({\n    'id': test_ids,\n    'y': ensemble_pred_test\n})\n\nprint(\"üëÄ Submission preview:\")\nprint(submission.head(10))\n\nprint(f\"\\nüìä Prediction statistics:\")\nprint(f\"   Mean: {ensemble_pred_test.mean():.4f}\")\nprint(f\"   Std: {ensemble_pred_test.std():.4f}\")\nprint(f\"   Min: {ensemble_pred_test.min():.4f}\")\nprint(f\"   Max: {ensemble_pred_test.max():.4f}\")\n\n# Save submission\nsubmission.to_csv('submission.csv', index=False)\nprint(\"\\n‚úÖ Submission saved as 'submission.csv'\")\n\n# ==============================================================================\n# MODEL INSIGHTS\n# ==============================================================================\n\nprint(\"\\nüîç MODEL INSIGHTS\")\nprint(\"=\" * 50)\n\n# Feature importance from the best model\nif hasattr(best_model, 'feature_importances_'):\n    feature_importance = pd.DataFrame({\n        'feature': X_train_scaled.columns,\n        'importance': best_model.feature_importances_\n    }).sort_values('importance', ascending=False)\n    \n    print(\"üîù Top 15 most important features:\")\n    print(feature_importance.head(15))\n    \n    # Plot feature importance\n    plt.figure(figsize=(10, 8))\n    sns.barplot(data=feature_importance.head(15), x='importance', y='feature')\n    plt.title(f'Top 15 Feature Importance ({best_model_name})')\n    plt.xlabel('Importance')\n    plt.tight_layout()\n    plt.show()\n\n# Model comparison visualization\nplt.figure(figsize=(10, 6))\nmodels_list = list(model_scores.keys()) + ['Ensemble']\nscores_list = list(model_scores.values()) + [ensemble_roc_auc]\ncolors = sns.color_palette(\"husl\", len(models_list))\n\nbars = plt.bar(models_list, scores_list, color=colors)\nplt.title('Model Performance Comparison (ROC AUC)')\nplt.ylabel('ROC AUC Score')\nplt.xticks(rotation=45)\nplt.ylim(0.5, max(scores_list) * 1.1)\n\n# Add value labels on bars\nfor bar, score in zip(bars, scores_list):\n    plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01, \n             f'{score:.4f}', ha='center', va='bottom')\n\nplt.tight_layout()\nplt.show()\n\nprint(\"\\nüéâ Analysis complete! Ready for submission to Kaggle.\")\nprint(\"üìÅ Files created:\")\nprint(\"   - submission.csv (your predictions)\")\nprint(\"\\nüí° Tips for improvement:\")\nprint(\"   - Try hyperparameter tuning\")\nprint(\"   - Feature engineering\")\nprint(\"   - Advanced ensemble methods\")\nprint(\"   - Use the original Bank Marketing dataset for additional training\")","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}